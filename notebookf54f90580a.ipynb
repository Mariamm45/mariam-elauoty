{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4089822,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# -------------------------------\n# Step 1: Load & Preprocess Dataset\n# -------------------------------\n\nDATASET_PATH = \"/kaggle/input/sign-language-recognition\"  # Your Kaggle dataset path\nIMG_SIZE = 64  # Resize images to 64x64\nSEQUENCE_LENGTH = 10  # Number of frames per sequence\nNUM_CLASSES = 10  # Adjust based on the number of sign classes\n\ndef load_video_frames(video_path):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    \n    while len(frames) < SEQUENCE_LENGTH:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))  # Resize to 64x64\n        frames.append(frame)\n    \n    cap.release()\n    \n    # If video has fewer frames, pad with last frame\n    while len(frames) < SEQUENCE_LENGTH:\n        frames.append(frames[-1])\n    \n    return np.array(frames)\n\n# Load dataset\nX, y = [], []\nlabels = os.listdir(DATASET_PATH)  # Each folder is a label (A, B, C, etc.)\n\nlabel_dict = {label: i for i, label in enumerate(labels)}  # Encode labels\n\nfor label in labels:\n    label_path = os.path.join(DATASET_PATH, label)\n    for video in os.listdir(label_path):\n        video_path = os.path.join(label_path, video)\n        frames = load_video_frames(video_path)\n        X.append(frames)\n        y.append(label_dict[label])\n\n# Convert to NumPy arrays\nX = np.array(X).reshape(-1, SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 1)  # Reshape for CNN input\ny = to_categorical(y, num_classes=NUM_CLASSES)  # One-hot encode labels\n\n# Split dataset into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# -------------------------------\n# Step 2: Build CNN-LSTM Model\n# -------------------------------\n\nmodel = Sequential([\n    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 1)),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Flatten()),\n    LSTM(128, return_sequences=False),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(NUM_CLASSES, activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display model summary\nmodel.summary()\n\n# -------------------------------\n# Step 3: Train the Model\n# -------------------------------\n\nEPOCHS = 20\nBATCH_SIZE = 16\n\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n\n# -------------------------------\n# Step 4: Evaluate the Model\n# -------------------------------\n\nloss, acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {acc * 100:.2f}%\")\n\n# -------------------------------\n# Step 5: Save the Model\n# -------------------------------\n\nmodel.save(\"/kaggle/working/sign_language_cnn_lstm.h5\")\nprint(\"Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T19:39:56.654029Z","iopub.execute_input":"2025-03-20T19:39:56.654302Z","iopub.status.idle":"2025-03-20T19:40:15.908700Z","shell.execute_reply.started":"2025-03-20T19:39:56.654276Z","shell.execute_reply":"2025-03-20T19:40:15.906920Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-10d5b6326e0e>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mlabel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_video_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/kaggle/input/sign-language-recognition/__results__.html'"],"ename":"NotADirectoryError","evalue":"[Errno 20] Not a directory: '/kaggle/input/sign-language-recognition/__results__.html'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# -------------------------------\n# Step 1: Load & Preprocess Dataset\n# -------------------------------\n\nDATASET_PATH = \"/kaggle/input/sign-language-recognition\"  # Your Kaggle dataset path\nIMG_SIZE = 64  # Resize images to 64x64\nSEQUENCE_LENGTH = 10  # Number of frames per sequence\n\n# Get only label directories, ignore files\nlabels = [d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))]\nNUM_CLASSES = len(labels)  # Dynamically set the number of classes\n\nlabel_dict = {label: i for i, label in enumerate(labels)}  # Encode labels\n\ndef load_video_frames(video_path):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    \n    while len(frames) < SEQUENCE_LENGTH:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))  # Resize to 64x64\n        frames.append(frame)\n    \n    cap.release()\n    \n    # If video has fewer frames, pad with last frame\n    while len(frames) < SEQUENCE_LENGTH:\n        frames.append(frames[-1])\n    \n    return np.array(frames)\n\n# Load dataset\nX, y = [], []\n\nfor label in labels:\n    label_path = os.path.join(DATASET_PATH, label)\n    if not os.path.isdir(label_path):  # Ensure it's a directory\n        continue\n    \n    for video in os.listdir(label_path):\n        video_path = os.path.join(label_path, video)\n        if not video_path.endswith(('.mp4', '.avi', '.mov')):  # Only process video files\n            continue\n        \n        frames = load_video_frames(video_path)\n        X.append(frames)\n        y.append(label_dict[label])\n\n# Convert to NumPy arrays\nX = np.array(X).reshape(-1, SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 1)  # Reshape for CNN input\ny = to_categorical(y, num_classes=NUM_CLASSES)  # One-hot encode labels\n\n# Split dataset into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# -------------------------------\n# Step 2: Build CNN-LSTM Model\n# -------------------------------\n\nmodel = Sequential([\n    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 1)),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n    TimeDistributed(MaxPooling2D((2, 2))),\n    TimeDistributed(Flatten()),\n    LSTM(128, return_sequences=False),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(NUM_CLASSES, activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display model summary\nmodel.summary()\n\n# -------------------------------\n# Step 3: Train the Model\n# -------------------------------\n\nEPOCHS = 20\nBATCH_SIZE = 16\n\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n\n# -------------------------------\n# Step 4: Evaluate the Model\n# -------------------------------\n\nloss, acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {acc * 100:.2f}%\")\n\n# -------------------------------\n# Step 5: Save the Model\n# -------------------------------\n\nmodel.save(\"/kaggle/working/sign_language_cnn_lstm.h5\")\nprint(\"Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T19:41:16.392587Z","iopub.execute_input":"2025-03-20T19:41:16.392931Z","iopub.status.idle":"2025-03-20T19:41:16.447922Z","shell.execute_reply.started":"2025-03-20T19:41:16.392907Z","shell.execute_reply":"2025-03-20T19:41:16.446485Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-223a089c15e1>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Split dataset into train/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."],"ename":"ValueError","evalue":"With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, TimeDistributed, Flatten, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\n\n# تعريف نموذج CNN-LSTM\ndef create_cnn_lstm_model(input_shape, num_classes):\n    model = Sequential()\n\n    # طبقات CNN\n    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n    model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n    model.add(TimeDistributed(Flatten()))\n\n    # طبقات LSTM\n    model.add(LSTM(128, return_sequences=True))\n    model.add(Dropout(0.5))\n    model.add(LSTM(64))\n    model.add(Dropout(0.5))\n\n    # طبقات Fully Connected\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # تجميع النموذج\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n# تحضير البيانات (هذا مثال، يجب استبداله ببياناتك الخاصة)\ndef load_example_data():\n    # مثال: إنشاء بيانات وهمية\n    num_samples = 100  # عدد العينات\n    num_frames = 30    # عدد الإطارات في كل فيديو\n    height, width, channels = 64, 64, 3  # أبعاد الإطارات\n    num_classes = 10   # عدد الفئات (العلامات)\n\n    X_train = np.random.rand(num_samples, num_frames, height, width, channels)  # بيانات تدريب\n    y_train = np.random.randint(0, num_classes, num_samples)  # تسميات تدريب\n\n    X_test = np.random.rand(num_samples // 2, num_frames, height, width, channels)  # بيانات اختبار\n    y_test = np.random.randint(0, num_classes, num_samples // 2)  # تسميات اختبار\n\n    # تحويل التسميات إلى one-hot encoding\n    y_train = to_categorical(y_train, num_classes)\n    y_test = to_categorical(y_test, num_classes)\n\n    return X_train, X_test, y_train, y_test\n\n# تحميل البيانات\nX_train, X_test, y_train, y_test = load_example_data()\n\n# تعريف شكل المدخلات وعدد الفئات\ninput_shape = (None, 64, 64, 3)  # (عدد الإطارات, ارتفاع, عرض, قنوات)\nnum_classes = 10\n\n# إنشاء النموذج\nmodel = create_cnn_lstm_model(input_shape, num_classes)\n\n# تدريب النموذج\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# تقييم النموذج\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Loss: {loss}')\nprint(f'Test Accuracy: {accuracy}')\n\n# حفظ النموذج\nmodel.save('sign_language_cnn_lstm.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T19:43:35.917032Z","iopub.execute_input":"2025-03-20T19:43:35.917433Z","iopub.status.idle":"2025-03-20T20:07:29.764310Z","shell.execute_reply.started":"2025-03-20T19:43:35.917402Z","shell.execute_reply":"2025-03-20T20:07:29.762772Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 21s/step - accuracy: 0.0891 - loss: 2.4063 - val_accuracy: 0.1000 - val_loss: 2.3248\nEpoch 2/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 14s/step - accuracy: 0.1307 - loss: 2.3275 - val_accuracy: 0.0400 - val_loss: 2.3226\nEpoch 3/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14s/step - accuracy: 0.1027 - loss: 2.3678 - val_accuracy: 0.0400 - val_loss: 2.3243\nEpoch 4/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14s/step - accuracy: 0.1089 - loss: 2.3903 - val_accuracy: 0.0400 - val_loss: 2.3177\nEpoch 5/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 14s/step - accuracy: 0.1508 - loss: 2.2664 - val_accuracy: 0.0400 - val_loss: 2.3098\nEpoch 6/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 14s/step - accuracy: 0.0983 - loss: 2.3159 - val_accuracy: 0.0200 - val_loss: 2.3067\nEpoch 7/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14s/step - accuracy: 0.1598 - loss: 2.2842 - val_accuracy: 0.1000 - val_loss: 2.3068\nEpoch 8/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 14s/step - accuracy: 0.0930 - loss: 2.3849 - val_accuracy: 0.1000 - val_loss: 2.3073\nEpoch 9/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 14s/step - accuracy: 0.0766 - loss: 2.3957 - val_accuracy: 0.1000 - val_loss: 2.3050\nEpoch 10/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 22s/step - accuracy: 0.1178 - loss: 2.3395 - val_accuracy: 0.1000 - val_loss: 2.3111\nEpoch 11/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14s/step - accuracy: 0.1051 - loss: 2.3220 - val_accuracy: 0.1000 - val_loss: 2.3178\nEpoch 12/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14s/step - accuracy: 0.1452 - loss: 2.2941 - val_accuracy: 0.1000 - val_loss: 2.3203\nEpoch 13/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14s/step - accuracy: 0.1385 - loss: 2.3455 - val_accuracy: 0.1000 - val_loss: 2.3208\nEpoch 14/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 14s/step - accuracy: 0.1322 - loss: 2.2969 - val_accuracy: 0.1000 - val_loss: 2.3148\nEpoch 15/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 14s/step - accuracy: 0.1322 - loss: 2.3266 - val_accuracy: 0.1000 - val_loss: 2.3136\nEpoch 16/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 16s/step - accuracy: 0.1108 - loss: 2.2877 - val_accuracy: 0.1000 - val_loss: 2.3114\nEpoch 17/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 14s/step - accuracy: 0.0858 - loss: 2.3003 - val_accuracy: 0.1000 - val_loss: 2.2999\nEpoch 18/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 14s/step - accuracy: 0.1074 - loss: 2.3132 - val_accuracy: 0.1000 - val_loss: 2.2909\nEpoch 19/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 23s/step - accuracy: 0.1108 - loss: 2.2969 - val_accuracy: 0.1000 - val_loss: 2.2911\nEpoch 20/20\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 14s/step - accuracy: 0.1484 - loss: 2.3094 - val_accuracy: 0.1000 - val_loss: 2.2945\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759ms/step - accuracy: 0.0875 - loss: 2.2956\nTest Loss: 2.294471025466919\nTest Accuracy: 0.10000000149011612\n","output_type":"stream"}],"execution_count":3}]}